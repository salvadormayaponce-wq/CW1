# -*- coding: utf-8 -*-
"""CW Data.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Gu93gHoZvdulvIm_DMSgqjQTmojTx_uF
"""

#import the libraries that we are going to need to carry out the analysis
import numpy as np
import pandas as pd
import pandas_datareader as dr
#Define the url
import requests
sp500_url = 'https://en.wikipedia.org/wiki/List_of_S%26P_500_companies'
header = {
    "User-Agent": "Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/50.0.2661.75 Safari/537.36",
    "X-Requested-With": "XMLHttpRequest"
}

r = requests.get(sp500_url, headers=header)

data_table = pd.read_html(r.text, match="Symbol")[0]

data_table



data_table['Symbol']

import yfinance as yf
!pip install yahoofinancials
from yahoofinancials import YahooFinancials
from datetime import datetime
#define a start date and End Date
start = '2022-01-01'
end = '2025-01-01'
##Read Stock Price Data
Data_stock_price = yf.download(data_table['Symbol'].values.tolist(), start= start , end = end)

Data_stock_price

#Calculate the daily returns and annual volatility of the stock
import math
daily_returns = Data_stock_price['Close'].pct_change()
annual_volatility = daily_returns.std()*math.sqrt(252)

annual_volatility

import yfinance as yf
import pandas as pd

#introduce the function
def betas(markets, stocks, start_date, end_date):
    #download the historical data for the index/market
    market = yf.download(markets, start_date, end_date)
    market['stock_name'] = markets
    #calculate daily returns
    market['daily_return'] = market['Close'].pct_change(1)
    #calculate standard deviation of the returns
    market_std = market['daily_return'].std()
    market.dropna(inplace=True)
    market = market[['Close', 'stock_name', 'daily_return']]

    #download the historical data for each stock and calculate its stdv
    frames = []
    stds = []
    for i in stocks:
        data = yf.download(i, start_date, end_date)
        data['stock_name'] = i
        data['daily_return'] = data['Close'].pct_change(1)
        data.dropna(inplace=True)
        data = data[['Close', 'stock_name', 'daily_return']]
        data_std = data['daily_return'].std()
        frames.append(data)
        stds.append(data_std)

    #for each stock calculate its correlation with index/market
    stock_correlation = []
    for i in frames:
        correlation = i['daily_return'].corr(market['daily_return'])
        stock_correlation.append(correlation)

    #calculate beta
    betas_output = []
    for b, i in zip(stock_correlation, stds):
        beta_calc = b * (i / market_std)
        betas_output.append(beta_calc)

    #form dataframe with the results
    dictionary = {stocks[e]: betas_output[e] for e in range(len(stocks))}
    dataframe = pd.DataFrame([dictionary]).T
    dataframe.reset_index(inplace=True)
    dataframe.rename(
        columns={"index": "Stock_Name", 0: "Beta"},
        inplace=True
    )
    return dataframe

betas_output = betas('^GSPC', data_table['Symbol'], '2022-01-01', '2025-01-01')

# Use the Beta results we already computed
beta_values = betas_output.copy()

# Clean and rename for consistency
beta_values = beta_values.rename(columns={"Stock_Name": "Symbol"})
beta_values = beta_values.dropna()

# Preview
beta_values.head()

from sklearn.preprocessing import StandardScaler

scaler_beta = StandardScaler()
beta_scaled = scaler_beta.fit_transform(beta_values[["Beta"]])

from scipy.cluster.hierarchy import dendrogram, linkage
import matplotlib.pyplot as plt

# Use Ward linkage
Z = linkage(beta_scaled, method="ward")

plt.figure(figsize=(10, 6))
dendrogram(Z, truncate_mode="lastp", p=30, leaf_rotation=90., leaf_font_size=10.)
plt.title("Dendrogram for Agglomerative Clustering (Beta Only)")
plt.xlabel("Cluster merged")
plt.ylabel("Distance")
plt.grid(True, alpha=0.3)
plt.show()

from scipy.cluster.hierarchy import fcluster

K_AGGLO = 2

agglom_clusters = fcluster(Z, K_AGGLO, criterion="maxclust")

beta_values["Agglo_Cluster"] = agglom_clusters

beta_values.head()

plt.figure(figsize=(9,6))

for c in sorted(beta_values["Agglo_Cluster"].unique()):
    sub = beta_values[beta_values["Agglo_Cluster"] == c]
    plt.scatter(sub["Beta"], np.zeros_like(sub["Beta"]), label=f"Cluster {c}")

plt.yticks([])
plt.xlabel("Beta")
plt.title("Agglomerative Clusters Using Beta")
plt.legend()
plt.grid(True, alpha=0.3)
plt.show()

agglo_profile = beta_values.groupby("Agglo_Cluster")[["Beta"]].mean()
agglo_profile

N_TICKERS = None

tickers_full = list(data_table['Symbol'])

if N_TICKERS is not None:
    tickers_used = tickers_full[:N_TICKERS]
else:
    tickers_used = tickers_full

print("Number of tickers used:", len(tickers_used))
print("First 10 tickers:", tickers_used[:10])

betas_df = betas('^GSPC', tickers_used, '2022-01-01', '2025-01-01')

betas_df = betas_df.rename(columns={"Stock_Name": "Symbol"})

betas_df.head()

close_prices_used = Data_stock_price['Close'][tickers_used]

daily_returns_used = close_prices_used.pct_change()

annual_volatility_used = daily_returns_used.std() * np.sqrt(252)

annual_volatility_df = annual_volatility_used.reset_index()
annual_volatility_df.columns = ["Symbol", "Annual_Volatility"]

annual_volatility_df.head()

final_dataframe = pd.merge(
    betas_df,
    annual_volatility_df,
    on="Symbol",
    how="inner"
)

final_dataframe = final_dataframe.replace([np.inf, -np.inf], np.nan)


final_dataframe.dropna(subset=["Beta", "Annual_Volatility"], inplace=True)

final_dataframe.reset_index(drop=True, inplace=True)

print("Rows after cleaning:", len(final_dataframe))
final_dataframe.head()

from sklearn.preprocessing import StandardScaler

X = final_dataframe[["Beta", "Annual_Volatility"]]

scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

X_scaled[:5]

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

inertia_values = []
K_range = range(2, 11)

for k in K_range:
    km = KMeans(n_clusters=k, random_state=42, n_init=10)
    km.fit(X_scaled)
    inertia_values.append(km.inertia_)

plt.figure(figsize=(7,5))
plt.plot(list(K_range), inertia_values, marker='o')
plt.title("Elbow Method: Inertia vs K")
plt.xlabel("Number of Clusters (K)")
plt.ylabel("Inertia")
plt.grid(True)
plt.show()

from sklearn.cluster import KMeans

#determine K value
K_CHOSEN = 4

kmeans_final = KMeans(n_clusters=K_CHOSEN, random_state=42, n_init=10)
clusters = kmeans_final.fit_predict(X_scaled)

# Add cluster labels to final_dataframe
final_dataframe["Cluster"] = clusters

final_dataframe.head()

cluster_profiles = final_dataframe.groupby("Cluster")[["Beta", "Annual_Volatility"]].agg(["mean", "min", "max", "count"])
cluster_profiles

import matplotlib.pyplot as plt
import numpy as np
from scipy.spatial import ConvexHull

plt.figure(figsize=(9,6))

# Get clusters in order
cluster_ids = sorted(final_dataframe["Cluster"].unique())

# Map numeric clusters
letters = ["A", "B", "C", "D"]
cluster_label_map = {cid: letters[i] for i, cid in enumerate(cluster_ids)}

# Color palette
cmap = plt.cm.get_cmap("tab10", len(cluster_ids))

for j, cid in enumerate(cluster_ids):
    subset = final_dataframe[final_dataframe["Cluster"] == cid]
    x = subset["Beta"].values
    y = subset["Annual_Volatility"].values

    # Scatter points
    plt.scatter(
        x, y,
        s=40, alpha=0.75,
        color=cmap(j),
        label=f"Cluster {cluster_label_map[cid]}"
    )

    # Convex hull border
    if len(subset) >= 3:
        points = np.column_stack((x, y))
        hull = ConvexHull(points)
        hull_points = points[hull.vertices]
        hull_points = np.vstack([hull_points, hull_points[0]])

        plt.plot(
            hull_points[:,0], hull_points[:,1],
            color="black",
            linewidth=1,
            alpha=0.9
        )

plt.title("K-Means Clusters: Beta vs Annual Volatility (K=4)")
plt.xlabel("Beta")
plt.ylabel("Annual Volatility")
plt.legend(frameon=True)
plt.grid(True, alpha=0.25)
plt.show()